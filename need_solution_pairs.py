# -*- coding: utf-8 -*-
"""need-solution-pairs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xQGsj5ytCwiUj7-MuQcFUoCX_FqkhJXW
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import warnings

warnings.simplefilter(action='ignore')
# !pip install ipython-autotime
# # %load_ext autotime

# !pip install EbookLib beautifulsoup4 lxml yake
# !python -m spacy download en_core_web_sm
#nlp = spacy.load('en_core_web_sm')
# !pip install keybert nltk spacy

# Importing libraries
import pandas as pd
import os
import traceback
import zipfile
from datetime import datetime
import datetime as dt
from math import ceil
import sys
import numpy as np
import time
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import yake
from gensim.models import Word2Vec
import networkx as nx
import matplotlib.pyplot as plt
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import string

#!pip install wordcloud

"""# Step 1: Data collection and loading (User Generated Content Scraping)"""

def extract_and_clean_text_from_epub(epub_path):
    book = epub.read_epub(epub_path)
    text = ''

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            soup = BeautifulSoup(item.get_content(), 'html.parser')
            raw_text = soup.get_text()
            # Clean the text to remove excessive whitespaces
            cleaned_text = re.sub(r'\s+', ' ', raw_text).strip()
            text += cleaned_text + ' '

    return text

def save_text_to_file(text, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(text)

#epub_directory = '/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/DATASET 17 Recovery Books/'
#sum([i.endswith('.epub') for i in os.listdir(epub_directory)])

# Directory containing EPUB files
#epub_directory = "/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/DATASET 17 Recovery Books/"
epub_directory = "/data_books.zip"

# Directory to save the output text files
#output_directory = "/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/Kamaneeya's Work/Need-solution pairs/data/"
output_directory = "/output"

if not os.path.exists(output_directory):
    os.makedirs(output_directory)

# Loop through each EPUB file in the directory
for epub_file in os.listdir(epub_directory):
    if epub_file.endswith('.epub'):
        epub_path = os.path.join(epub_directory, epub_file)

        # Extract and clean the text
        book_text = extract_and_clean_text_from_epub(epub_path)

        # Specify the output file path
        output_file_name = os.path.splitext(epub_file)[0] + '_cleaned.txt'
        output_file_path = os.path.join(output_directory, output_file_name)

        # Save the cleaned text to a file
        save_text_to_file(book_text, output_file_path)

        print(f"Processed and saved: {output_file_name}")

quit()

"""# Step 2A: Innovation Concept Identification UNIGRAM

1. Preprocess Text: Tokenizes, removes stopwords, and lemmatizes the text from each book.
2. Keyword Extraction with YAKE: Identifies key terms from the preprocessed text.
3. Train Word2Vec Model: Generates word embeddings from the tokenized text of all books.
4. Create Semantic Network: Builds a network of keywords based on their semantic similarity from the Word2Vec model.
5. Visualize: Draws the semantic network to identify clusters of related innovation concepts.

This workflow integrates preprocessing, keyword extraction, and semantic network analytics to identify and visualize innovation concepts within a corpus of text documents.
"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Preprocessing function
def preprocess_text(text):
    tokens = word_tokenize(text)
    tokens = [w.lower() for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if not w in stop_words]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return tokens

# Load and preprocess text data
directory_path = output_directory
tokenized_docs = []
for filename in os.listdir(directory_path):
    if filename.endswith('.txt'):
        file_path = os.path.join(directory_path, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            tokens = preprocess_text(text)
            tokenized_docs.append(tokens)

# Train Word2Vec model
model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Adjust YAKE to extract unigrams
kw_extractor = yake.KeywordExtractor(n=1, top=20)  # Extract top 20 unigrams

# Keyword extraction (adjust according to your needs)
keywords = set()
for doc in tokenized_docs:
    doc_text = ' '.join(doc)
    extracted_keywords = kw_extractor.extract_keywords(doc_text)
    for kw, _ in extracted_keywords:
        keywords.add(kw.split()[0])  # Assuming unigrams, but safe-guarding split

# Create a semantic network
G = nx.Graph()
for word in keywords:
    if word in model.wv:
        G.add_node(word)
        for other_word in keywords:
            if other_word in model.wv and other_word != word:
                similarity = model.wv.similarity(word, other_word)
                if similarity > 0.5:  # Example threshold
                    G.add_edge(word, other_word, weight=similarity)

if len(G.nodes) == 0:
    print("The graph G has no nodes.")
else:
    print(f"The graph G has {len(G.nodes)} nodes.")

if len(G.edges) == 0:
    print("The graph G has no edges.")
else:
    print(f"The graph G has {len(G.edges)} edges.")

#extracted_keywords

# Print a sample of the Word2Vec model's vocabulary
for i, word in enumerate(model.wv.index_to_key):
    if i >= 10: break  # Limit to first 10 words
    print(word)

single_word_keywords = [kw[0] for kw in extracted_keywords]

# Now, check if these single words are in the model's vocabulary
for keyword in single_word_keywords[:10]:  # Check the first 10 keywords
    if keyword in model.wv:
        print(f"'{keyword}' is in the model's vocabulary.")
    else:
        print(f"'{keyword}' is NOT in the model's vocabulary.")

# Assuming G is your original graph with 465 nodes and 48700 edges

# Step 1: Define a weight threshold for edge filtering
weight_threshold = 0.7  # Adjust this threshold based on your edge weights

# Filter edges by weight
edges_to_keep = [(u, v) for u, v, d in G.edges(data=True) if d.get('weight', 0) > weight_threshold]
G_filtered_by_weight = nx.Graph()
G_filtered_by_weight.add_edges_from(edges_to_keep)

# Add nodes to ensure the graph's integrity
for node in G.nodes():
    if not G_filtered_by_weight.has_node(node):
        G_filtered_by_weight.add_node(node)

# Step 2: Optionally, further filter nodes based on degree or other criteria
# For example, removing nodes with degree less than a threshold
degree_threshold = 130  # Adjust this threshold based on your criteria
nodes_to_remove = [node for node, degree in dict(G_filtered_by_weight.degree()).items() if degree < degree_threshold]
G_final_filtered = G_filtered_by_weight.copy()
G_final_filtered.remove_nodes_from(nodes_to_remove)

# Visualization of the final filtered graph with adjusted labels
pos = nx.spring_layout(G_final_filtered)  # Use the same position layout as before

# Calculate node degrees
degrees = dict(G_final_filtered.degree())

# Scale node sizes by degree (you might need to adjust the scaling factor)
node_sizes = [degrees[node] * 10 for node in G_final_filtered.nodes()]  # Adjust scaling factor as needed

# Visualization of the final filtered graph
pos = nx.spring_layout(G_final_filtered)  # Adjust layout as necessary
plt.figure(figsize=(20, 12))
nx.draw_networkx(G_final_filtered, pos, with_labels=False, node_size=node_sizes, edge_color='lightgray', alpha=0.8)  # Adjust visual parameters as needed
# Draw labels with adjusted positions and make them black
nx.draw_networkx_labels(G_final_filtered,pos, font_color='black')
plt.show()

"""- Nodes represent key innovation concepts.
- Edges demonstrate semantic relationships, indicating how closely related or contextually similar these concepts are to each other.
- Clusters of nodes, especially denser ones, highlight thematic concentrations.

Interpreting such a plot helps identify central themes in your data, understand the relationship between different innovation concepts, and spot potential areas for further exploration or development.
"""

#Save the Word2Vec Model
model.save(output_directory + "word2vec_model.model")

#Save the Semantic Network
import json
from networkx.readwrite import json_graph

G_data = json_graph.node_link_data(G)  # Convert G to a dict suitable for JSON
for k,v in G_data.items(): # iterate through key, value of dict 'd'
  if k =="links":
    for i in range(len(v)):
      v[i]['weight'] = float(v[i]['weight'])
with open(output_directory + "semantic_network.json", "w") as f:
    json.dump(G_data, f)

"""## Cluster and dimensionality reduction"""

from sklearn.cluster import KMeans
import numpy as np

# Assuming `keywords` is a list of relevant words present in the Word2Vec model
word_vectors = np.array([model.wv[word] for word in keywords if word in model.wv])

# K-means clustering
num_clusters = 5  # Adjust based on your analysis
kmeans = KMeans(n_clusters=num_clusters)
kmeans.fit(word_vectors)

# Example of using the cluster labels
for idx, word in enumerate(keywords):
    if word in model.wv:
        print(f"Word: {word}, Cluster: {kmeans.labels_[idx]}")

from sklearn.manifold import TSNE

# Dimensionality reduction with t-SNE
tsne = TSNE(n_components=2)  # Reduce to 2D for visualization
word_vectors_2d = tsne.fit_transform(word_vectors)

# Plotting the results
plt.figure(figsize=(20, 10))
plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])
for i, word in enumerate(keywords):
    if word in model.wv:
        plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))
plt.show()

#import spacy

# Load spaCy model
nlp = spacy.load("en_core_web_sm")  # or "en_core_web_lg" for a more comprehensive model

# Process a sample text
doc = nlp("Your sample text here.")

# Dependency parsing and named entity recognition example
for token in doc:
    print(f"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}")
for ent in doc.ents:
    print(f"Entity: {ent.text}, Type: {ent.label_}")

"""Word: "Your", Dependency: "poss" (possessive), Head: "text"

"Your" is a possessive modifier of "text". It indicates possession, showing that "text" belongs to "your".
Word: "sample", Dependency: "compound", Head: "text"

"sample" is part of a compound noun with "text". "Compound" means that two or more words are combined to form a single concept, in this case, "sample text".
Word: "text", Dependency: "ROOT", Head: "text"

"text" is the root of the sentence, meaning it's the central word to which other words are connected. In dependency grammar, the root is the main verb or action, but in sentences like this example, the noun can serve as the root if it's the sentence's focus.
Word: "here", Dependency: "advmod" (adverbial modifier), Head: "text"

"here" modifies "text", specifying the location or context of the text. As an adverbial modifier, it provides additional information about the circumstances of the action or state described by the verb or sentence.
Word: ".", Dependency: "punct" (punctuation), Head: "text"

The period is marked as punctuation, indicating the end of the sentence, with "text" being the central point it's connected to.
Interpreting spaCy's output helps in understanding the syntactic structure of sentences, which can be crucial for tasks like information extraction, question answering, and more nuanced natural language understanding efforts.
"""



"""# Step 2B: Innovation Concept Identification BIGRAM

1. Preprocess Text: Tokenizes, removes stopwords, and lemmatizes the text from each book.
2. Keyword Extraction with YAKE: Identifies key terms from the preprocessed text.
3. Train Word2Vec Model: Generates word embeddings from the tokenized text of all books.
4. Create Semantic Network: Builds a network of keywords based on their semantic similarity from the Word2Vec model.
5. Visualize: Draws the semantic network to identify clusters of related innovation concepts.

This workflow integrates preprocessing, keyword extraction, and semantic network analytics to identify and visualize innovation concepts within a corpus of text documents.
"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Preprocessing function
def preprocess_text(text):
    tokens = word_tokenize(text)
    tokens = [w.lower() for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if not w in stop_words]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return tokens

# Load and preprocess text data
directory_path = output_directory
tokenized_docs = []
for filename in os.listdir(directory_path):
    if filename.endswith('.txt'):
        file_path = os.path.join(directory_path, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            tokens = preprocess_text(text)
            tokenized_docs.append(tokens)

# Train Word2Vec model
model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Adjust YAKE to extract unigrams
kw_extractor = yake.KeywordExtractor(n=2, top=20)  # Extract top 20 bigrams

# Keyword extraction (adjust according to your needs)
keywords = set()
for doc in tokenized_docs:
    doc_text = ' '.join(doc)
    extracted_keywords = kw_extractor.extract_keywords(doc_text)
    for kw, _ in extracted_keywords:
        keywords.add(kw)

# Create a semantic network
G = nx.Graph()
for word in keywords:
    if word in model.wv:
        G.add_node(word)
        for other_word in keywords:
            if other_word in model.wv and other_word != word:
                similarity = model.wv.similarity(word, other_word)
                if similarity > 0.5:  # Example threshold
                    G.add_edge(word, other_word, weight=similarity)

if len(G.nodes) == 0:
    print("The graph G has no nodes.")
else:
    print(f"The graph G has {len(G.nodes)} nodes.")

if len(G.edges) == 0:
    print("The graph G has no edges.")
else:
    print(f"The graph G has {len(G.edges)} edges.")

extracted_keywords

# Print a sample of the Word2Vec model's vocabulary
for i, word in enumerate(model.wv.index_to_key):
    if i >= 10: break  # Limit to first 10 words
    print(word)

# Step 1: Define a weight threshold for edge filtering
weight_threshold = 0.7  # Adjust this threshold based on your edge weights

# Filter edges by weight
edges_to_keep = [(u, v) for u, v, d in G.edges(data=True) if d.get('weight', 0) > weight_threshold]
G_filtered_by_weight = nx.Graph()
G_filtered_by_weight.add_edges_from(edges_to_keep)

# Add nodes to ensure the graph's integrity
for node in G.nodes():
    if not G_filtered_by_weight.has_node(node):
        G_filtered_by_weight.add_node(node)

# Step 2: Optionally, further filter nodes based on degree or other criteria
# For example, removing nodes with degree less than a threshold
degree_threshold = 130  # Adjust this threshold based on your criteria
nodes_to_remove = [node for node, degree in dict(G_filtered_by_weight.degree()).items() if degree < degree_threshold]
G_final_filtered = G_filtered_by_weight.copy()
G_final_filtered.remove_nodes_from(nodes_to_remove)

# Visualization of the final filtered graph with adjusted labels
pos = nx.spring_layout(G_final_filtered)  # Use the same position layout as before

# Calculate node degrees
degrees = dict(G_final_filtered.degree())

# Scale node sizes by degree (you might need to adjust the scaling factor)
node_sizes = [degrees[node] * 10 for node in G_final_filtered.nodes()]  # Adjust scaling factor as needed

# Visualization of the final filtered graph
pos = nx.spring_layout(G_final_filtered)  # Adjust layout as necessary
plt.figure(figsize=(20, 12))
nx.draw_networkx(G_final_filtered, pos, with_labels=False, node_size=node_sizes, edge_color='lightgray', alpha=0.8)  # Adjust visual parameters as needed
# Draw labels with adjusted positions and make them black
nx.draw_networkx_labels(G_final_filtered,pos, font_color='black')
plt.show()

"""# Step 2C: Innovation Concept Identification TRIGRAM

1. Preprocess Text: Tokenizes, removes stopwords, and lemmatizes the text from each book.
2. Keyword Extraction with YAKE: Identifies key terms from the preprocessed text.
3. Train Word2Vec Model: Generates word embeddings from the tokenized text of all books.
4. Create Semantic Network: Builds a network of keywords based on their semantic similarity from the Word2Vec model.
5. Visualize: Draws the semantic network to identify clusters of related innovation concepts.

This workflow integrates preprocessing, keyword extraction, and semantic network analytics to identify and visualize innovation concepts within a corpus of text documents.
"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Preprocessing function
def preprocess_text(text):
    tokens = word_tokenize(text)
    tokens = [w.lower() for w in tokens if w.isalpha()]
    tokens = [w for w in tokens if not w in stop_words]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    return tokens

# Load and preprocess text data
directory_path = output_directory
tokenized_docs = []
for filename in os.listdir(directory_path):
    if filename.endswith('.txt'):
        file_path = os.path.join(directory_path, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            tokens = preprocess_text(text)
            tokenized_docs.append(tokens)

# Train Word2Vec model
model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)

# Adjust YAKE to extract unigrams
kw_extractor = yake.KeywordExtractor(n=3, top=20)  # Extract top 20 trigrams

# Keyword extraction (adjust according to your needs)
keywords = set()
for doc in tokenized_docs:
    doc_text = ' '.join(doc)
    extracted_keywords = kw_extractor.extract_keywords(doc_text)
    for kw, _ in extracted_keywords:
        keywords.add(kw)
# Create a semantic network
G = nx.Graph()
for word in keywords:
    if word in model.wv:
        G.add_node(word)
        for other_word in keywords:
            if other_word in model.wv and other_word != word:
                similarity = model.wv.similarity(word, other_word)
                if similarity > 0.5:  # Example threshold
                    G.add_edge(word, other_word, weight=similarity)

if len(G.nodes) == 0:
    print("The graph G has no nodes.")
else:
    print(f"The graph G has {len(G.nodes)} nodes.")

if len(G.edges) == 0:
    print("The graph G has no edges.")
else:
    print(f"The graph G has {len(G.edges)} edges.")

extracted_keywords

# Print a sample of the Word2Vec model's vocabulary
for i, word in enumerate(model.wv.index_to_key):
    if i >= 10: break  # Limit to first 10 words
    print(word)

# Assuming G is your original graph with 465 nodes and 48700 edges

# Step 1: Define a weight threshold for edge filtering
weight_threshold = 0.7  # Adjust this threshold based on your edge weights

# Filter edges by weight
edges_to_keep = [(u, v) for u, v, d in G.edges(data=True) if d.get('weight', 0) > weight_threshold]
G_filtered_by_weight = nx.Graph()
G_filtered_by_weight.add_edges_from(edges_to_keep)

# Add nodes to ensure the graph's integrity
for node in G.nodes():
    if not G_filtered_by_weight.has_node(node):
        G_filtered_by_weight.add_node(node)

# Step 2: Optionally, further filter nodes based on degree or other criteria
# For example, removing nodes with degree less than a threshold
degree_threshold = 130  # Adjust this threshold based on your criteria
nodes_to_remove = [node for node, degree in dict(G_filtered_by_weight.degree()).items() if degree < degree_threshold]
G_final_filtered = G_filtered_by_weight.copy()
G_final_filtered.remove_nodes_from(nodes_to_remove)

# Visualization of the final filtered graph with adjusted labels
pos = nx.spring_layout(G_final_filtered)  # Use the same position layout as before

# Calculate node degrees
degrees = dict(G_final_filtered.degree())

# Scale node sizes by degree (you might need to adjust the scaling factor)
node_sizes = [degrees[node] * 10 for node in G_final_filtered.nodes()]  # Adjust scaling factor as needed

# Visualization of the final filtered graph
pos = nx.spring_layout(G_final_filtered)  # Adjust layout as necessary
plt.figure(figsize=(20, 12))
nx.draw_networkx(G_final_filtered, pos, with_labels=False, node_size=node_sizes, edge_color='lightgray', alpha=0.8)  # Adjust visual parameters as needed
# Draw labels with adjusted positions and make them black
nx.draw_networkx_labels(G_final_filtered,pos, font_color='black')
plt.show()

"""# Step 2D: Innovation Concept Identification UNIGRAM

1. Load preprocessed data
2. Keyword Extraction with KeyBERT
"""

from keybert import KeyBERT
from spacy.lang.en.stop_words import STOP_WORDS
import string
import spacy
import os

def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize and lemmatize
    doc = nlp(text.lower())
    lemmatized = [token.lemma_ for token in doc if token.text not in STOP_WORDS and token.text not in string.punctuation]
    return " ".join(lemmatized)

# Assuming preprocessing function `preprocess_text` is defined as above
# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize KeyBERT model
kw_model = KeyBERT()

# Directory containing your preprocessed text files
directory_path = "/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/Kamaneeya's Work/Need-solution pairs/data/"

# Dictionary to hold keywords for each document
document_keywords = {}

# Iterate over each file in the directory
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)

    # Ensure we're only processing text files
    if filename.endswith(".txt"):
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            processed_text = preprocess_text(text)  # Preprocess the text

            # Extract keywords with KeyBERT
            keywords = kw_model.extract_keywords(processed_text, keyphrase_ngram_range=(1, 1), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)

            # Store the keywords in the dictionary
            document_keywords[filename] = keywords

# Now `document_keywords` contains the keywords for each document

document_keywords

from collections import Counter

# Aggregate keywords
all_keywords = []
for keywords in document_keywords.values():
    for keyword, _ in keywords:
        all_keywords.append(keyword)

# Count the frequency of each keyword
keyword_frequency = Counter(all_keywords)

# Prepare data for the word cloud (word cloud library expects a dictionary)
wordcloud_data = {word: count for word, count in keyword_frequency.items()}

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate a word cloud image
wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(wordcloud_data)

# Display the generated image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()



"""# Step 2E: Innovation Concept Identification BIGRAM

1. Load preprocessed data
2. Keyword Extraction with KeyBERT
"""

from keybert import KeyBERT
from spacy.lang.en.stop_words import STOP_WORDS
import string
import spacy
import os

def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize and lemmatize
    doc = nlp(text.lower())
    lemmatized = [token.lemma_ for token in doc if token.text not in STOP_WORDS and token.text not in string.punctuation]
    return " ".join(lemmatized)

# Assuming preprocessing function `preprocess_text` is defined as above
# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize KeyBERT model
kw_model = KeyBERT()

# Directory containing your preprocessed text files
directory_path = "/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/Kamaneeya's Work/Need-solution pairs/data/"

# Dictionary to hold keywords for each document
document_keywords = {}

# Iterate over each file in the directory
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)

    # Ensure we're only processing text files
    if filename.endswith(".txt"):
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            processed_text = preprocess_text(text)  # Preprocess the text

            # Extract keywords with KeyBERT
            keywords = kw_model.extract_keywords(processed_text, keyphrase_ngram_range=(2, 2), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)

            # Store the keywords in the dictionary
            document_keywords[filename] = keywords

# Now `document_keywords` contains the keywords for each document

document_keywords

from collections import Counter

# Aggregate keywords
all_keywords = []
for keywords in document_keywords.values():
    for keyword, _ in keywords:
        all_keywords.append(keyword)

# Count the frequency of each keyword
keyword_frequency = Counter(all_keywords)

# Prepare data for the word cloud (word cloud library expects a dictionary)
wordcloud_data = {word: count for word, count in keyword_frequency.items()}

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate a word cloud image
wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(wordcloud_data)

# Display the generated image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()



"""# Step 2F: Innovation Concept Identification TRIGRAM

1. Load preprocessed data
2. Keyword Extraction with KeyBERT
"""

from keybert import KeyBERT
from spacy.lang.en.stop_words import STOP_WORDS
import string
import spacy
import os

def preprocess_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize and lemmatize
    doc = nlp(text.lower())
    lemmatized = [token.lemma_ for token in doc if token.text not in STOP_WORDS and token.text not in string.punctuation]
    return " ".join(lemmatized)

# Assuming preprocessing function `preprocess_text` is defined as above
# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize KeyBERT model
kw_model = KeyBERT()

# Directory containing your preprocessed text files
directory_path = "/content/gdrive/Shared drives/Biomotivate Sensor Data Shared Folder/Kamaneeya's Work/Need-solution pairs/data/"

# Dictionary to hold keywords for each document
document_keywords = {}

# Iterate over each file in the directory
for filename in os.listdir(directory_path):
    file_path = os.path.join(directory_path, filename)

    # Ensure we're only processing text files
    if filename.endswith(".txt"):
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            processed_text = preprocess_text(text)  # Preprocess the text

            # Extract keywords with KeyBERT
            keywords = kw_model.extract_keywords(processed_text, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)

            # Store the keywords in the dictionary
            document_keywords[filename] = keywords

# Now `document_keywords` contains the keywords for each document

document_keywords

from collections import Counter

# Aggregate keywords
all_keywords = []
for keywords in document_keywords.values():
    for keyword, _ in keywords:
        all_keywords.append(keyword)

# Count the frequency of each keyword
keyword_frequency = Counter(all_keywords)

# Prepare data for the word cloud (word cloud library expects a dictionary)
wordcloud_data = {word: count for word, count in keyword_frequency.items()}

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate a word cloud image
wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate_from_frequencies(wordcloud_data)

# Display the generated image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

